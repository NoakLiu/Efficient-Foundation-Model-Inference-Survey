# Efficient-Large-Foundation-Model-Inference-A-Perspective-From-Model-and-System-Co-Design
Paper Link: https://arxiv.org/abs/2409.01990

## Model Design

### Quantization
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **On-policy distillation of language models: Learning from self-generated mistakes** | 2024 | Knowledge Distillation | [Link](https://openreview.net/forum?id=OdnNBxlShZG) |
| **Flamingo: A visual language model for few-shot learning** | 2022 | Visual Language Models | [Link](https://arxiv.org/abs/2204.14198) |
| **Memory use of GPT-J 6B** | 2021 | Memory Management | [Link](https://discuss.huggingface.co/t/memory-use-of-gpt-j-6b/10078) |
| **SparseGPT: Massive language models can be accurately pruned in one-shot** | 2023 | Model Pruning | [Link](https://arxiv.org/abs/2303.12712) |

### Distillation
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **Fluctuation-based adaptive structured pruning for large language models** | 2024 | Structured Pruning | [Link](https://arxiv.org/abs/2406.02069) |
| **Quarot: Outlier-free 4-bit inference in rotated LLMs** | 2024 | Quantization | [Link](https://arxiv.org/abs/2404.00456) |
| **Token merging: Your ViT but faster** | 2023 | Efficient Transformers | [Link](http://arxiv.org/abs/2210.09461) |
| **FlashAttention: Fast and memory-efficient exact attention with I/O-awareness** | 2022 | Attention Mechanisms | [Link](https://arxiv.org/abs/2205.14135) |

### Pruning
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **PyramidKV: Dynamic KV cache compression based on pyramidal information funneling** | 2024 | KV Cache Compression | [Link](https://arxiv.org/abs/2406.02069) |
| **Knowledge distillation for closed-source language models** | 2024 | Knowledge Distillation | [Link](https://arxiv.org/abs/2401.07013) |
| **Sparse-quantized representation for near-lossless LLM weight compression** | 2023 | Model Compression | [Link](https://arxiv.org/abs/2306.03078) |
| **Pruner-zero: Evolving symbolic pruning metric from scratch for large language models** | 2024 | Pruning Techniques | [Link](https://arxiv.org/abs/2406.02924) |

## System Design

### K-V Cache
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **Quest: Query-aware sparsity for long-context transformers** | 2024 | Efficient Transformers | [Link](https://arxiv.org/abs/2401.07013) |
| **Hessian-based optimization for multi-modal attention models** | 2022 | Optimization Techniques | [Link](https://arxiv.org/abs/2406.03792) |

### Cache Eviction
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **SparseGPT: Pruning GPT models effectively in one shot** | 2023 | Model Pruning | [Link](https://arxiv.org/abs/2205.14135) |
| **FlashAttention: Fast attention with low memory footprint** | 2023 | Memory Management | [Link](https://arxiv.org/abs/2205.14135) |

### Memory Management
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **ZipVL: Efficient large vision-language models with dynamic token sparsification and KV cache compression** | 2024 | Memory Management | [Link](https://arxiv.org/abs/2410.08584) |
| **Low-rank adaptation for cross-modal learning** | 2021 | Cross-modal Learning | [Link](https://arxiv.org/abs/2106.09685) |

### MoE
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **Light-PEFT: Lightening parameter-efficient fine-tuning via early pruning** | 2024 | Fine-Tuning | [Link](https://arxiv.org/abs/2406.03792) |
| **Minillm: Knowledge distillation of large language models** | 2024 | Knowledge Distillation | [Link](https://openreview.net/forum?id=OdnNBxlShZG) |

## Model-Sys Co-Design

### Mixed Precision Training
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **Power-BERT: Accelerating BERT inference via progressive word-vector elimination** | 2020 | Efficient Inference | [Link](https://arxiv.org/abs/2005.12345) |
| **Learning both weights and connections for efficient neural network** | 2015 | Neural Network Efficiency | [Link](https://arxiv.org/abs/1803.03635) |

### Fine-Tuning with System Optimization
| Article Title | Year | Subfield | Link |
| --- | --- | --- | --- |
| **Effective methods for improving interpretability in reinforcement learning models** | 2021 | Reinforcement Learning | [Link](https://link.springer.com/article/10.1007/s10462-021-09984-x) |
| **Sparse-pruning for accelerating transformer models** | 2023 | Pruning and Optimization | [Link](https://arxiv.org/abs/2305.14135) |
